## TD3
TD3 somehow does not learn for the full mdp. It keeps getting stuck in policies that keep performing an action on the boundary of the observation space. Then, after ca. 100-200 episodes, it randomly flips a value from the boundary and get's stuck on the other side of the boundary. What could be the reason? Maybe vanishing gradients because of tanh output activation? But why does is it still able to flip then?  
The Q-values do look kind of reasonable. After a while though, they start to diverge in the negative direction, which is very unusual for Q-learning with function approximation. The reason might be the twin Q-networks used in TD3, where the lower evaluation is chosen as a target.
Torch Batch normalization behaves very weird and seemingly produces completely different results after calling model.eval(). This kind of makes sense, because it switches from batch-mean to running mean, but the fact that the results are completely different makes it kind of useless in evalutation.